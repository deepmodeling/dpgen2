import os, sys, json
package_root = r'/home/runner/work/dpgen2/dpgen2/tests/block-54cbr/iter-002--run-lmp-000001/workdir/tmp/inputs/artifacts/dflow_python_packages'
catalog_dir = os.path.join(package_root, '.dflow')
if os.path.exists(catalog_dir):
    for f in os.listdir(catalog_dir):
        with open(os.path.join(catalog_dir, f), 'r') as fd:
            for item in json.load(fd)['path_list']:
                path = os.path.join(package_root, os.path.dirname(item['dflow_list_item']))
                sys.path.insert(0, path)
                os.environ['PYTHONPATH'] = path + ':' + os.environ.get('PYTHONPATH', '')
import json
from dflow import config, jsonpickle, s3_config
config.update(jsonpickle.loads(r'''{"host": "https://127.0.0.1:2746", "namespace": "argo", "token": null, "k8s_config_file": null, "k8s_api_server": null, "private_key_host_path": null, "save_path_as_parameter": false, "catalog_dir_name": ".dflow", "archive_mode": "tar", "util_image": "python:3.8", "util_image_pull_policy": null, "extender_image": "dptechnology/dflow-extender", "extender_image_pull_policy": null, "dispatcher_image": "dptechnology/dpdispatcher", "dispatcher_image_pull_policy": null, "save_keys_in_global_outputs": false, "mode": "debug", "lineage": null, "register_tasks": false, "http_headers": {}, "workflow_annotations": {}, "overwrite_reused_artifact": true, "detach": false, "debug_copy_method": "symlink", "debug_pool_workers": null, "debug_batch_size": null, "debug_batch_interval": 30, "detect_empty_dir": true, "artifact_register": {"bohrium+datasets": "dflow.plugins.bohrium.BohriumDatasetsArtifact"}, "debug_s3": false, "debug_workdir": ".", "debug_artifact_dir": ".", "debug_failfast": false, "debug_save_copy_method": "symlink", "raise_for_group": false, "dispatcher_debug": false, "dereference_symlink": false}'''))
s3_config.update(jsonpickle.loads(r'''{"endpoint": "127.0.0.1:9000", "console": "http://127.0.0.1:9001", "access_key": "admin", "secret_key": "password", "secure": false, "bucket_name": "my-bucket", "repo_key": null, "repo": null, "repo_type": "s3", "repo_prefix": "", "prefix": "", "storage_client": null, "extra_prefixes": []}'''))

import os, sys, traceback
from dflow.python import OPIO, TransientError, FatalError
from dflow.python.utils import handle_input_artifact, handle_input_parameter
from dflow.python.utils import handle_output_artifact, handle_output_parameter, handle_lineage
from mocked_ops import MockedRunLmp

op_obj = MockedRunLmp()
op_obj.key = 'iter-002--run-lmp-000001'
if op_obj.key.startswith('{'): op_obj.key = None
op_obj.workflow_name = 'block-54cbr'
if __name__ == '__main__':
    input = OPIO()
    input_sign = MockedRunLmp.get_input_sign()
    output_sign = MockedRunLmp.get_output_sign()
    input['config'] = handle_input_parameter('config', r'''{}''', input_sign['config'], None, r'/home/runner/work/dpgen2/dpgen2/tests/block-54cbr/iter-002--run-lmp-000001/workdir/tmp')
    input['task_name'] = handle_input_parameter('task_name', r'''["task.000000", "task.000001", "task.000002", "task.000003", "task.000004", "task.000005"]''', input_sign['task_name'], int('000001'), r'/home/runner/work/dpgen2/dpgen2/tests/block-54cbr/iter-002--run-lmp-000001/workdir/tmp')
    input['task_path'] = handle_input_artifact('task_path', input_sign['task_path'], int('000001'), r'/home/runner/work/dpgen2/dpgen2/tests/block-54cbr/iter-002--run-lmp-000001/workdir/tmp', None, n_parts=None, keys_of_parts=None, prefix=None)
    input['models'] = handle_input_artifact('models', input_sign['models'], None, r'/home/runner/work/dpgen2/dpgen2/tests/block-54cbr/iter-002--run-lmp-000001/workdir/tmp', None, n_parts=None, keys_of_parts=None, prefix=None)
    op_obj.tmp_root = '/home/runner/work/dpgen2/dpgen2/tests/block-54cbr/iter-002--run-lmp-000001/workdir/tmp'
    op_obj.create_slice_dir = False
    op_obj.slices['log'] = int('000001')
    op_obj.slices['traj'] = int('000001')
    op_obj.slices['model_devi'] = int('000001')
    op_obj.slices['plm_output'] = int('000001')
    op_obj.slices['optional_output'] = int('000001')
    op_obj.slices['extra_outputs'] = int('000001')
    op_obj.pool_size = None
    import signal
    def sigterm_handler(signum, frame):
        print('Got SIGTERM')
        raise RuntimeError('Got SIGTERM')
    signal.signal(signal.SIGTERM, sigterm_handler)
    try:
        try:
            output = op_obj.execute(input)
        except Exception as e:
            if op_obj.outputs:
                op_obj.handle_outputs(op_obj.outputs)
            raise e
    except TransientError:
        traceback.print_exc()
        sys.exit(1)
    except FatalError:
        traceback.print_exc()
        sys.exit(2)
    op_obj.handle_outputs(output)
