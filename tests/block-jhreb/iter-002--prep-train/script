import os, sys, json
package_root = r'/home/runner/work/dpgen2/dpgen2/tests/block-jhreb/iter-002--prep-train/workdir/tmp/inputs/artifacts/dflow_python_packages'
catalog_dir = os.path.join(package_root, '.dflow')
if os.path.exists(catalog_dir):
    for f in os.listdir(catalog_dir):
        with open(os.path.join(catalog_dir, f), 'r') as fd:
            for item in json.load(fd)['path_list']:
                path = os.path.join(package_root, os.path.dirname(item['dflow_list_item']))
                sys.path.insert(0, path)
                os.environ['PYTHONPATH'] = path + ':' + os.environ.get('PYTHONPATH', '')
import json
from dflow import config, jsonpickle, s3_config
config.update(jsonpickle.loads(r'''{"host": "https://127.0.0.1:2746", "namespace": "argo", "token": null, "k8s_config_file": null, "k8s_api_server": null, "private_key_host_path": null, "save_path_as_parameter": false, "catalog_dir_name": ".dflow", "archive_mode": "tar", "util_image": "python:3.8", "util_image_pull_policy": null, "extender_image": "dptechnology/dflow-extender", "extender_image_pull_policy": null, "dispatcher_image": "dptechnology/dpdispatcher", "dispatcher_image_pull_policy": null, "save_keys_in_global_outputs": false, "mode": "debug", "lineage": null, "register_tasks": false, "http_headers": {}, "workflow_annotations": {}, "overwrite_reused_artifact": true, "detach": false, "debug_copy_method": "symlink", "debug_pool_workers": null, "debug_batch_size": null, "debug_batch_interval": 30, "detect_empty_dir": true, "artifact_register": {"bohrium+datasets": "dflow.plugins.bohrium.BohriumDatasetsArtifact"}, "debug_s3": false, "debug_workdir": ".", "debug_artifact_dir": ".", "debug_failfast": false, "debug_save_copy_method": "symlink", "raise_for_group": false, "dispatcher_debug": false, "dereference_symlink": false}'''))
s3_config.update(jsonpickle.loads(r'''{"endpoint": "127.0.0.1:9000", "console": "http://127.0.0.1:9001", "access_key": "admin", "secret_key": "password", "secure": false, "bucket_name": "my-bucket", "repo_key": null, "repo": null, "repo_type": "s3", "repo_prefix": "", "prefix": "", "storage_client": null, "extra_prefixes": []}'''))

import os, sys, traceback
from dflow.python import OPIO, TransientError, FatalError
from dflow.python.utils import handle_input_artifact, handle_input_parameter
from dflow.python.utils import handle_output_artifact, handle_output_parameter, handle_lineage
from mocked_ops import MockedPrepDPTrain

op_obj = MockedPrepDPTrain()
op_obj.key = 'iter-002--prep-train'
if op_obj.key.startswith('{'): op_obj.key = None
op_obj.workflow_name = 'block-jhreb'
if __name__ == '__main__':
    input = OPIO()
    input_sign = MockedPrepDPTrain.get_input_sign()
    output_sign = MockedPrepDPTrain.get_output_sign()
    input['template_script'] = handle_input_parameter('template_script', r'''{"seed": 1024, "data": []}''', input_sign['template_script'], None, r'/home/runner/work/dpgen2/dpgen2/tests/block-jhreb/iter-002--prep-train/workdir/tmp')
    input['numb_models'] = handle_input_parameter('numb_models', r'''3''', input_sign['numb_models'], None, r'/home/runner/work/dpgen2/dpgen2/tests/block-jhreb/iter-002--prep-train/workdir/tmp')
    op_obj.tmp_root = '/home/runner/work/dpgen2/dpgen2/tests/block-jhreb/iter-002--prep-train/workdir/tmp'
    op_obj.create_slice_dir = False
    op_obj.slices['task_names'] = None
    op_obj.slices['task_paths'] = None
    op_obj.pool_size = None
    import signal
    def sigterm_handler(signum, frame):
        print('Got SIGTERM')
        raise RuntimeError('Got SIGTERM')
    signal.signal(signal.SIGTERM, sigterm_handler)
    try:
        try:
            output = op_obj.execute(input)
        except Exception as e:
            if op_obj.outputs:
                op_obj.handle_outputs(op_obj.outputs)
            raise e
    except TransientError:
        traceback.print_exc()
        sys.exit(1)
    except FatalError:
        traceback.print_exc()
        sys.exit(2)
    op_obj.handle_outputs(output)
